{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG-19 cifar-10",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ojas29092000/cifar10/blob/main/VGG_19_cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a20YEO6yzuui"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "# print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-L0qzDtz6kM"
      },
      "source": [
        "#Import standard libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "''' to learn more about itertools visit\n",
        "    https://medium.com/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8'''\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYj7yYF_0Bfr"
      },
      "source": [
        "#Import keras functions\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "'''Since we are using transfer learning let's import the model that we want to implement.Let's use VGG 19(19 layers) and Resnet-50 (50 layers of residual units). \n",
        "Residual units allow us to add more layers onto the model without a degradation in accuracy.\n",
        "Let's try and compare the accuracy of the 2 models and see if the addtional layers do make a significant difference. '''\n",
        "\n",
        "from tensorflow.keras.applications import VGG19,ResNet50\n",
        "\n",
        "'Import the datagenerator to augment images'\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "'''Import the optimizers and leanring rate annealer (which will reduce the learning rate once a particular metric we choose(in this case validation error) \n",
        "does not reduce after a user defined number of epochs)'''\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "'Lastly import the final layers that will be added on top of the base model'\n",
        "from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n",
        "\n",
        "'Import to_categorical from the keras utils package to one hot encode the labels'\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTLDW1zt0Rbg"
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGYhJpL70uvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687e8033-36e6-4dda-abc5-717c28d98df3"
      },
      "source": [
        "#Divide the data in Train, Validation and Test Datasets\n",
        "'I had to turn the Internet setting to on to download load the dataset'\n",
        "(x_train,y_train),(x_test,y_test)=cifar10.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNlT91Ug0yAg"
      },
      "source": [
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77zMu7aS02gY",
        "outputId": "fdbe196a-2662-4e9d-e95c-b9a73f5c5dd1"
      },
      "source": [
        "#Print the dimensions of the datasets to make sure everything's kosher\n",
        "\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((35000, 32, 32, 3), (35000, 1))\n",
            "((15000, 32, 32, 3), (15000, 1))\n",
            "((10000, 32, 32, 3), (10000, 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMU78Aq305Sg"
      },
      "source": [
        "#One hot encode the labels.Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\n",
        "\n",
        "y_train=to_categorical(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UygveqbI08sf",
        "outputId": "8c964414-5945-422b-c2c1-c04422975031"
      },
      "source": [
        "# Lets print the dimensions one more time to see if things changed the way we expected\n",
        "\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((35000, 32, 32, 3), (35000, 10))\n",
            "((15000, 32, 32, 3), (15000, 10))\n",
            "((10000, 32, 32, 3), (10000, 10))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E0XEYNV1DUP"
      },
      "source": [
        "#Data Augmentation Function: Let's define an instance of the ImageDataGenerator class and set the parameters.We have to instantiate for the Train,Validation and Test datasets\n",
        "train_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2, \n",
        "                                    horizontal_flip=True,\n",
        "                                    zoom_range=.1 )\n",
        "\n",
        "val_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2, \n",
        "                                    horizontal_flip=True,\n",
        "                                    zoom_range=.1)\n",
        "\n",
        "test_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2, \n",
        "                                    horizontal_flip= True,\n",
        "                                    zoom_range=.1) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X0MB7P31Irv"
      },
      "source": [
        "#Fit the augmentation method to the data\n",
        "\n",
        "train_generator.fit(x_train)\n",
        "val_generator.fit(x_val)\n",
        "test_generator.fit(x_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDZJITTb1L1g"
      },
      "source": [
        "'''Learning Rate Annealer: The learning rate can be modified after a set number of epochs or after a certain condition is met. We will use the latter and change the learning rate if \n",
        "the validation error does not reduce after a set number of epochs. To do this we will use the patience parameter.'''\n",
        "\n",
        "lrr= ReduceLROnPlateau(\n",
        "                       monitor='val_acc', #Metric to be measured\n",
        "                       factor=.01, #Factor by which learning rate will be reduced\n",
        "                       patience=3,  #No. of epochs after which if there is no improvement in the val_acc, the learning rate is reduced\n",
        "                       min_lr=1e-5) #The minimum learning rate "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYKfKlxN1QNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ebcc95-883e-4860-8375-778c79ae6103"
      },
      "source": [
        "#Build the model\n",
        "\n",
        "'The first base model used is VGG19. The pretrained weights from the imagenet challenge are used'\n",
        "base_model_1 = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])\n",
        "'For the 2nd base model we will use Resnet 50 and compare the performance against the previous one.The hypothesis is that Resnet 50 should perform better because of its deeper architecture'\n",
        "base_model_2 = ResNet50(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZnUbOvt1VXo"
      },
      "source": [
        "#Lets add the final layers to these base models where the actual classification is done in the dense layers\n",
        "\n",
        "model_1= Sequential()\n",
        "model_1.add(base_model_1) #Adds the base model (in this case vgg19 to model_1)\n",
        "model_1.add(Flatten()) #Since the output before the flatten layer is a matrix we have to use this function to get a vector of the form nX1 to feed it into the fully connected layers"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AisDks_61aWQ",
        "outputId": "4db84b5f-7b7a-404d-a7c0-7c0d1e75bfd4"
      },
      "source": [
        "model_1.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,024,384\n",
            "Trainable params: 20,024,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Gavpxs1dWP"
      },
      "source": [
        "#Add the Dense layers along with activation and batch normalization\n",
        "model_1.add(Dense(1024,activation=('relu'),input_dim=512))\n",
        "model_1.add(Dense(512,activation=('relu'))) \n",
        "model_1.add(Dense(256,activation=('relu'))) \n",
        "#model_1.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\n",
        "model_1.add(Dense(128,activation=('relu')))\n",
        "#model_1.add(Dropout(.2))\n",
        "model_1.add(Dense(10,activation=('softmax'))) #This is the classification layer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE69BKLo1h9j",
        "outputId": "b900bae3-ad1c-4ec7-d11a-bcc9a06daaad"
      },
      "source": [
        "#Check final model summary\n",
        "model_1.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              525312    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,240,010\n",
            "Trainable params: 21,240,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U8GjK-c1kiL"
      },
      "source": [
        "batch_size= 100\n",
        "epochs=50"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewf4w-Dr1prR",
        "outputId": "2c72bf7f-535d-48c8-f164-bda67c62ce24"
      },
      "source": [
        "learn_rate=.001\n",
        "\n",
        "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
        "adam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75LundkM1uya"
      },
      "source": [
        "#Compile the model\n",
        "#During model compiling the 3 main things we specify are loss function,optimizer and the metrics that need to be evaluated during the test and train processes.\n",
        "#Lets start by using the SGD optimizer\n",
        "#We will specify the loss as categoricl crossentropy since the labels are 1 hot encoded. IF we had integer labels,we'd have to use sparse categorical crossentropy as loss function.\n",
        "model_1.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noBSnBog17H5"
      },
      "source": [
        "#Compile the model\n",
        "#During model compiling the 3 main things we specify are loss function,optimizer and the metrics that need to be evaluated during the test and train processes.\n",
        "#Lets start by using the SGD optimizer\n",
        "#We will specify the loss as categoricl crossentropy since the labels are 1 hot encoded. IF we had integer labels,we'd have to use sparse categorical crossentropy as loss function.\n",
        "model_1.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7imxuuYb4bG5",
        "outputId": "4c8eed8e-eea1-40e0-b608-886aa704e3da"
      },
      "source": [
        "model_1.fit_generator(train_generator.flow(x_train,y_train,batch_size=batch_size),\n",
        "                      epochs=epochs,\n",
        "                      steps_per_epoch=x_train.shape[0]//batch_size,\n",
        "                      validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),validation_steps=250,\n",
        "                      callbacks=[lrr],verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 2.1442 - accuracy: 0.1983WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 250 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
            "350/350 [==============================] - 88s 159ms/step - loss: 2.1442 - accuracy: 0.1983 - val_loss: 1.8384 - val_accuracy: 0.3365 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 1.3495 - accuracy: 0.5110WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 1.3495 - accuracy: 0.5110 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.8415 - accuracy: 0.7113WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.8415 - accuracy: 0.7113 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.7655WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.6877 - accuracy: 0.7655 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.5851 - accuracy: 0.8024WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.5851 - accuracy: 0.8024 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.8228WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.5194 - accuracy: 0.8228 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.8398WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.4637 - accuracy: 0.8398 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8546WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.4223 - accuracy: 0.8546 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8683WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 127ms/step - loss: 0.3807 - accuracy: 0.8683 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.8827WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.3368 - accuracy: 0.8827 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.3112 - accuracy: 0.8922WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.3112 - accuracy: 0.8922 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9030WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.2792 - accuracy: 0.9030 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9131WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.2560 - accuracy: 0.9131 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9206WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.2291 - accuracy: 0.9206 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9291WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.2020 - accuracy: 0.9291 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.1893 - accuracy: 0.9346WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.1893 - accuracy: 0.9346 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.9416WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.1697 - accuracy: 0.9416 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9443WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.1618 - accuracy: 0.9443 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9508WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.1417 - accuracy: 0.9508 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9579WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.1218 - accuracy: 0.9579 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9563WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.1254 - accuracy: 0.9563 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9647WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.1021 - accuracy: 0.9647 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9712WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.0906 - accuracy: 0.9712 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9691WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.0916 - accuracy: 0.9691 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9720WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0838 - accuracy: 0.9720 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9718WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0819 - accuracy: 0.9718 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9765WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0702 - accuracy: 0.9765 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9791WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0624 - accuracy: 0.9791 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9811WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0571 - accuracy: 0.9811 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9806WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.0577 - accuracy: 0.9806 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9813WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0554 - accuracy: 0.9813 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9835WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0483 - accuracy: 0.9835 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9856WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0438 - accuracy: 0.9856 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9880WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0367 - accuracy: 0.9880 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9847WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.0449 - accuracy: 0.9847 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9857WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 128ms/step - loss: 0.0436 - accuracy: 0.9857 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9883WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0358 - accuracy: 0.9883 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9867WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0382 - accuracy: 0.9867 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9894WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0344 - accuracy: 0.9894 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9901WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0293 - accuracy: 0.9901 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9881WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0351 - accuracy: 0.9881 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9893WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0325 - accuracy: 0.9893 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9920WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0239 - accuracy: 0.9920 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9921WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0231 - accuracy: 0.9921 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9925WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0227 - accuracy: 0.9925 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9897WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0310 - accuracy: 0.9897 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9912WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0258 - accuracy: 0.9912 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9911WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0258 - accuracy: 0.9911 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9939WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0193 - accuracy: 0.9939 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "350/350 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9940WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,lr\n",
            "350/350 [==============================] - 45s 129ms/step - loss: 0.0193 - accuracy: 0.9940 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f658be70710>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN_EMRa6EaRc"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rns1_taWDtYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c64c38d-7a93-43bb-c71c-3525642c6339"
      },
      "source": [
        "scores = model_1.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 9s 26ms/step - loss: 0.7862 - accuracy: 0.8582\n",
            "Test loss: 0.786201536655426\n",
            "Test accuracy: 0.8582000136375427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOZ73kdLEdA_"
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "#     print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}